{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88b9b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:08.274615Z",
     "start_time": "2022-02-09T02:55:01.383344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import os\n",
    "from os import listdir\n",
    "import re\n",
    "import bisect\n",
    "import random\n",
    "# import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from Model_General import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle5\n",
    "import pandas as pd\n",
    "from anytree import Node, RenderTree,NodeMixin\n",
    "import heapq\n",
    "import re\n",
    "from matplotlib.pyplot import hist\n",
    "from ngram_fusion_util import *\n",
    "from ProactiveUtil import *\n",
    "from utilgeneral import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d34dab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:08.289547Z",
     "start_time": "2022-02-09T02:55:08.276553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dda34f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:11.008082Z",
     "start_time": "2022-02-09T02:55:08.290515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing double time data...\n",
      "time min 21.0\n",
      "time min 21.0\n",
      "saved mapped user time map parameters\n",
      "max time 1900794.0000000002\n",
      "using util general Preparing done... timenum and ratio 1900773 0.5\n",
      "User: 23127, Item: 13582:, Timenum: 1900773\n",
      "average sequence length: 5.17\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='Mapping_Result_ExceptTVseries_i3u3_1000000',type=str)\n",
    "parser.add_argument('--train_dir', default='default',type=str)\n",
    "parser.add_argument('--batch_size', default=128, type=int)\n",
    "parser.add_argument('--lr', default=0.001, type=float)\n",
    "parser.add_argument('--maxlen', default=53, type=int)\n",
    "parser.add_argument('--hidden_units', default=50, type=int)\n",
    "parser.add_argument('--num_blocks', default=2, type=int)\n",
    "parser.add_argument('--num_epochs', default=101, type=int)\n",
    "parser.add_argument('--num_heads', default=1, type=int)\n",
    "parser.add_argument('--time_span', default=18000, type=int)\n",
    "parser.add_argument('--dropout_rate', default=0.2, type=float)\n",
    "parser.add_argument('--l2_emb', default=0.00005, type=float)\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not os.path.isdir(args.dataset + '_' + args.train_dir):\n",
    "    os.makedirs(args.dataset + '_' + args.train_dir)\n",
    "with open(os.path.join(args.dataset + '_' + args.train_dir, 'args.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([str(k) + ',' + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n",
    "f.close()\n",
    "\n",
    "# dataset = data_partition_daysplit_doubletime(args.dataset,1900283)\n",
    "dataset = data_partition_daysplit_doubletime(args.dataset,0.5)\n",
    "# [user_train, user_valid, user_test, usernum, itemnum, timenum] = dataset\n",
    "[user_train, user_test, usernum, itemnum, timenum] = dataset\n",
    "print('User: %d, Item: %d:, Timenum: %d'%(usernum, itemnum, timenum))\n",
    "num_batch = int(len(user_train) / args.batch_size)\n",
    "cc = 0.0\n",
    "for u in user_train:\n",
    "    cc += len(user_train[u])\n",
    "print('average sequence length: %.2f' % (cc / len(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822d0949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:16.710043Z",
     "start_time": "2022-02-09T02:55:11.009050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/Modeliptvi3u3span18000len53-model.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = Model(usernum, itemnum, timenum,args)\n",
    "savernew = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "savernew.restore(sess, 'model/Modeliptvi3u3span18000len53-model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46632748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:17.419421Z",
     "start_time": "2022-02-09T02:55:16.713024Z"
    }
   },
   "outputs": [],
   "source": [
    "treepath = '../../data/iptvdata/'\n",
    "tree_file = 'NgramiptvExceptTVseries_i3u3_sample1000000N3.pkl'\n",
    "with open(treepath+tree_file, 'rb') as input:\n",
    "    treeroot = pickle5.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e77bbbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:19.827889Z",
     "start_time": "2022-02-09T02:55:17.420418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921772</th>\n",
       "      <td>2010-06-23 23:59:59</td>\n",
       "      <td>00111010001000111010010101100110</td>\n",
       "      <td>棒球大联盟第3季(第12集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170271</th>\n",
       "      <td>2010-06-24 00:00:00</td>\n",
       "      <td>00111010000100000100001100100110</td>\n",
       "      <td>棒球大联盟第1季(第26集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897261</th>\n",
       "      <td>2010-06-24 00:00:01</td>\n",
       "      <td>00111010001000110111111111010101</td>\n",
       "      <td>泡沫之夏(第03集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834023</th>\n",
       "      <td>2010-06-24 00:00:04</td>\n",
       "      <td>00111010001000110000111001011110</td>\n",
       "      <td>AIR(第01集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803235</th>\n",
       "      <td>2010-06-24 00:00:05</td>\n",
       "      <td>00111010001000101100101101111010</td>\n",
       "      <td>重案六组3(第01集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334475</th>\n",
       "      <td>2010-07-15 23:59:56</td>\n",
       "      <td>00111010000100001111000110111010</td>\n",
       "      <td>半路兄弟(第04集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668088</th>\n",
       "      <td>2010-07-15 23:59:56</td>\n",
       "      <td>00111010001000011000001001011000</td>\n",
       "      <td>樱桃小丸子(第01集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41302</th>\n",
       "      <td>2010-07-15 23:59:57</td>\n",
       "      <td>00111010000100000000001010000100</td>\n",
       "      <td>反抗之真心英雄(第25集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109893</th>\n",
       "      <td>2010-07-15 23:59:59</td>\n",
       "      <td>00111010000100000001110100111111</td>\n",
       "      <td>暮色3：月食(抢先版).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30347</th>\n",
       "      <td>2010-07-15 23:59:59</td>\n",
       "      <td>00111010000100000000001000100111</td>\n",
       "      <td>犬夜叉完结篇(第04集).mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time                              user  \\\n",
       "921772  2010-06-23 23:59:59  00111010001000111010010101100110   \n",
       "170271  2010-06-24 00:00:00  00111010000100000100001100100110   \n",
       "897261  2010-06-24 00:00:01  00111010001000110111111111010101   \n",
       "834023  2010-06-24 00:00:04  00111010001000110000111001011110   \n",
       "803235  2010-06-24 00:00:05  00111010001000101100101101111010   \n",
       "...                     ...                               ...   \n",
       "334475  2010-07-15 23:59:56  00111010000100001111000110111010   \n",
       "668088  2010-07-15 23:59:56  00111010001000011000001001011000   \n",
       "41302   2010-07-15 23:59:57  00111010000100000000001010000100   \n",
       "109893  2010-07-15 23:59:59  00111010000100000001110100111111   \n",
       "30347   2010-07-15 23:59:59  00111010000100000000001000100111   \n",
       "\n",
       "                      item  \n",
       "921772  棒球大联盟第3季(第12集).mp4  \n",
       "170271  棒球大联盟第1季(第26集).mp4  \n",
       "897261      泡沫之夏(第03集).mp4  \n",
       "834023       AIR(第01集).mp4  \n",
       "803235     重案六组3(第01集).mp4  \n",
       "...                    ...  \n",
       "334475      半路兄弟(第04集).mp4  \n",
       "668088     樱桃小丸子(第01集).mp4  \n",
       "41302    反抗之真心英雄(第25集).mp4  \n",
       "109893     暮色3：月食(抢先版).mp4  \n",
       "30347     犬夜叉完结篇(第04集).mp4  \n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following is loading testing dataset\n",
    "filepath = '../../data/iptvdata/'\n",
    "filename = 'iptv_sample1000000.csv'\n",
    "df = pd.read_csv(filepath+filename)\n",
    "df = df.sort_values(by=['time'],ascending = (True))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1744becf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:20.150997Z",
     "start_time": "2022-02-09T02:55:19.829855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262481</th>\n",
       "      <td>2010-07-05 00:00:00</td>\n",
       "      <td>00111010000100001011011101010111</td>\n",
       "      <td>宫心计国语版(第16集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591458</th>\n",
       "      <td>2010-07-05 00:00:00</td>\n",
       "      <td>00111010001000010000001110010111</td>\n",
       "      <td>加油!网球王子(第01集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982004</th>\n",
       "      <td>2010-07-05 00:00:02</td>\n",
       "      <td>00111010001001010000110000001011</td>\n",
       "      <td>无法阻挡的婚姻.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269294</th>\n",
       "      <td>2010-07-05 00:00:02</td>\n",
       "      <td>00111010000100001011111001100101</td>\n",
       "      <td>马英九为后ECFA时代台湾经济战略定调-7月3日.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663543</th>\n",
       "      <td>2010-07-05 00:00:03</td>\n",
       "      <td>00111010001000010111100110110110</td>\n",
       "      <td>风云雄霸天下(蓝光).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334475</th>\n",
       "      <td>2010-07-15 23:59:56</td>\n",
       "      <td>00111010000100001111000110111010</td>\n",
       "      <td>半路兄弟(第04集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668088</th>\n",
       "      <td>2010-07-15 23:59:56</td>\n",
       "      <td>00111010001000011000001001011000</td>\n",
       "      <td>樱桃小丸子(第01集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41302</th>\n",
       "      <td>2010-07-15 23:59:57</td>\n",
       "      <td>00111010000100000000001010000100</td>\n",
       "      <td>反抗之真心英雄(第25集).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109893</th>\n",
       "      <td>2010-07-15 23:59:59</td>\n",
       "      <td>00111010000100000001110100111111</td>\n",
       "      <td>暮色3：月食(抢先版).mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30347</th>\n",
       "      <td>2010-07-15 23:59:59</td>\n",
       "      <td>00111010000100000000001000100111</td>\n",
       "      <td>犬夜叉完结篇(第04集).mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                              user  \\\n",
       "262481 2010-07-05 00:00:00  00111010000100001011011101010111   \n",
       "591458 2010-07-05 00:00:00  00111010001000010000001110010111   \n",
       "982004 2010-07-05 00:00:02  00111010001001010000110000001011   \n",
       "269294 2010-07-05 00:00:02  00111010000100001011111001100101   \n",
       "663543 2010-07-05 00:00:03  00111010001000010111100110110110   \n",
       "...                    ...                               ...   \n",
       "334475 2010-07-15 23:59:56  00111010000100001111000110111010   \n",
       "668088 2010-07-15 23:59:56  00111010001000011000001001011000   \n",
       "41302  2010-07-15 23:59:57  00111010000100000000001010000100   \n",
       "109893 2010-07-15 23:59:59  00111010000100000001110100111111   \n",
       "30347  2010-07-15 23:59:59  00111010000100000000001000100111   \n",
       "\n",
       "                                item  \n",
       "262481              宫心计国语版(第16集).mp4  \n",
       "591458             加油!网球王子(第01集).mp4  \n",
       "982004                   无法阻挡的婚姻.mp4  \n",
       "269294  马英九为后ECFA时代台湾经济战略定调-7月3日.mp4  \n",
       "663543                风云雄霸天下(蓝光).mp4  \n",
       "...                              ...  \n",
       "334475                半路兄弟(第04集).mp4  \n",
       "668088               樱桃小丸子(第01集).mp4  \n",
       "41302              反抗之真心英雄(第25集).mp4  \n",
       "109893               暮色3：月食(抢先版).mp4  \n",
       "30347               犬夜叉完结篇(第04集).mp4  \n",
       "\n",
       "[555120 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset parameter:\n",
    "dataset_begintime = datetime(2010,6,24)\n",
    "ratio = 0.5\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "splittime = dataset_begintime + (df['time'].max()- dataset_begintime)*ratio\n",
    "df_test = df[df['time']>splittime]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10d4357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:22.557020Z",
     "start_time": "2022-02-09T02:55:20.151993Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilee\\anaconda3new\\envs\\temporal_3_6python\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test.dropna(inplace=True)\n",
    "# np.nan in list(['item'])\n",
    "df_test\n",
    "df_test.to_csv(filepath+'iptv_sample1000000testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f636b3a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:25.258268Z",
     "start_time": "2022-02-09T02:55:22.557891Z"
    }
   },
   "outputs": [],
   "source": [
    "#the following is loading mapping result\n",
    "itemmappingfilename = 'Item_Mapping_ExceptTVseries_i3u3_1000000.txt'\n",
    "usermappingfilename = 'User_Mapping_ExceptTVseries_i3u3_1000000.txt'\n",
    "itemmapping = pd.read_csv(filepath+itemmappingfilename,sep='\\t',names=['itemname','num'])\n",
    "usermapping = pd.read_csv(filepath+usermappingfilename,sep='\\t',names=['username','num'])\n",
    "usermappingdict = defaultdict(int)\n",
    "itemmappingdict = defaultdict(int)\n",
    "for index,user in usermapping.iterrows():\n",
    "    username = user[0]\n",
    "    userencode = user[1]\n",
    "    usermappingdict[username] = userencode\n",
    "    usermappingdict[userencode] = username\n",
    "for index,item in itemmapping.iterrows():\n",
    "    itemname = str(item[0])\n",
    "    itemencode = item[1]\n",
    "    itemmappingdict[itemname] = itemencode\n",
    "    itemmappingdict[itemencode] = itemname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4099ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60773221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:25.445517Z",
     "start_time": "2022-02-09T02:55:25.259158Z"
    }
   },
   "outputs": [],
   "source": [
    "#the followng is loading medium and std for each item\n",
    "medium_std_filename = '0to11days_iptv_median_std.txt'\n",
    "medium_std = pd.read_csv(filepath+medium_std_filename,sep='\\t',names=['item','medium','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea5bae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:25.646452Z",
     "start_time": "2022-02-09T02:55:25.446268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13., 11.,  8.,  7.,  8.,  4.,  6.,  0.,  0.,  0.]),\n",
       " array([100., 110., 120., 130., 140., 150., 160., 170., 180., 190., 200.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(medium_std['medium'],range=(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d3785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddec31c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.695364Z",
     "start_time": "2022-02-09T02:55:25.647338Z"
    }
   },
   "outputs": [],
   "source": [
    "medium_std_dict = defaultdict(list)\n",
    "for index,item in medium_std.iterrows():\n",
    "#     print(item[0],item[1],item[2],item[1:])\n",
    "#     w\n",
    "    medium_std_dict[item[0]] = [item[1],item[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e8ec21c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.710690Z",
     "start_time": "2022-02-09T02:55:34.696364Z"
    }
   },
   "outputs": [],
   "source": [
    "# medium_std_dict['绯闻少女第3季第17集(预告片).mp4'][0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237f8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bb943b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.726727Z",
     "start_time": "2022-02-09T02:55:34.711680Z"
    }
   },
   "outputs": [],
   "source": [
    "def isseries(item):\n",
    "    if len(item.split('(第')) >= 2 and len(item.split('(第')[1].split('集)')) >= 2 and item.split('(第')[1].split('集)')[0].isdigit():\n",
    "        return True\n",
    "    elif len(item)>3 and item[-1].isdigit() and not item[-4].isdigit() and item[-3]!='/' and item[-3]!=':' and item[-3:]!='mp4' and len(re.findall(r\"\\d+\", item[-3:]))>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f761b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.742586Z",
     "start_time": "2022-02-09T02:55:34.727649Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_series(item):\n",
    "    if len(item.split('(第')) >= 2 and len(item.split('(第')[1].split('集)')) >= 2 and item.split('(第')[1].split('集)')[0].isdigit():\n",
    "        episode_num = item.split('(第')[1].split('集)')[0]\n",
    "        predicted_series_item = item.split('(第')[0]+'(第'+re.sub(episode_num, str(int(episode_num)+1).zfill(len(episode_num)), episode_num)+'集)'+item.split('集)')[1]\n",
    "        return predicted_series_item\n",
    "    elif len(item)>3 and item[-1].isdigit() and not item[-4].isdigit() and item[-3]!='/' and item[-3]!=':' and item[-3:]!='mp4' and len(re.findall(r\"\\d+\", item[-3:]))>0:\n",
    "        episode_num = re.findall(r\"\\d+\", item[-3:])[0]\n",
    "        predicted_series_item = item[:-3]+re.sub(episode_num, str(int(episode_num)+1).zfill(len(episode_num)), item[-3:])\n",
    "        return predicted_series_item\n",
    "    else:\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2e8d1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.757546Z",
     "start_time": "2022-02-09T02:55:34.743580Z"
    }
   },
   "outputs": [],
   "source": [
    "# from ProactiveUtil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969e1729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:55:34.835310Z",
     "start_time": "2022-02-09T02:55:34.758540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PEC(CACHE_SIZE = 100, ratio = 0.9, lruN = 2,downloadtime = 0.5,printoutfrequency = 1000,begintime = datetime(2010,7,5), stop_time = datetime(2010,7,5,1)):\n",
    "    #the followsing is caching simulator\n",
    "#     CACHE_SIZE = 1000\n",
    "#     begintime = datetime(2010,7,5)\n",
    "    P_CACHE_SIZE = int(CACHE_SIZE*ratio)\n",
    "    R_CACHE_SIZE= CACHE_SIZE  - P_CACHE_SIZE\n",
    "#     downloadtime = 0.5\n",
    "#     printoutfrequency = 1000\n",
    "#     stop_point = 10000  \n",
    "    clear_point = 1000\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    ngram_dict = {}\n",
    "    ngram_dict_time = {}\n",
    "    ngram_dict_confidence = {}\n",
    "    N = 2 #N+1 gram\n",
    "#     lruN = 8\n",
    "    fusiontopk = 10\n",
    "    preditivemodeltopk = 20\n",
    "    guess_len = 20\n",
    "    SCORE_Norm = 10\n",
    "    total_count = 0\n",
    "    local_count = 0\n",
    "    proactive_cache = set()\n",
    "    reactive_cache = set()\n",
    "    hit = 0\n",
    "    accum_hit = 0\n",
    "    soft_hit = 0\n",
    "    proactive_download = 0\n",
    "    reactive_download_number = 0\n",
    "    hit_rate_list = []\n",
    "    Series_len = {}\n",
    "    Cache_score = {}\n",
    "    Prediction_score_dict = {}\n",
    "    Reactive_item_proactive_score = {}\n",
    "    user_contribution2item = {}\n",
    "    past_timestamps = {}\n",
    "    reactive_latency = 0\n",
    "    proactive_latency = 0\n",
    "    proactive_download_list = []\n",
    "    utilization_list_proactive = []\n",
    "    utilization_list_reactive = []\n",
    "    utilization_list_all = []\n",
    "    calculatetime = begintime\n",
    "\n",
    "    watch_ratio_est_list = []\n",
    "    watch_ratio_gt_list = []\n",
    "    we_have_nan = False\n",
    "\n",
    "\n",
    "    count_tvseries = 0\n",
    "    count_activeother = 0\n",
    "    count_other = 0\n",
    "    last_time = datetime.now()\n",
    "    updatetime = begintime\n",
    "    \n",
    "    updating_time_overhead_list = []\n",
    "    prediction_time_overhead_list = []\n",
    "\n",
    "#     updating_time_overhead_list = []\n",
    "    \n",
    "    bandwidth_link = {'begin time':[],'end time':[],'item':[],'proactive_indicator':[]}\n",
    "    print('p cache size:',P_CACHE_SIZE,'r cache size:',R_CACHE_SIZE,'frequency',printoutfrequency,'downloadtime',downloadtime,'lruN',lruN)\n",
    "    print('begintime',begintime,'stoptime',stop_time)\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        user = row[1]\n",
    "        time = row[0].to_pydatetime()#avoid <class 'pandas._libs.tslibs.timestamps.Timestamp'>, we need datetime\n",
    "    #     time = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "        item = row[2]\n",
    "    #     if len(item.split('(第'))>=2:\n",
    "    # #         print(item)\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         continue\n",
    "        if time < begintime:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if total_count == 0:\n",
    "            link_empty_time = time\n",
    "        total_count += 1\n",
    "        \n",
    "#         if total_count < 30000:\n",
    "#             continue\n",
    "            \n",
    "        local_count += 1\n",
    "        if item not in Prediction_score_dict and item not in Cache_score and item not in Reactive_item_proactive_score:\n",
    "            Prediction_score_dict[item] = []\n",
    "    #update link\n",
    "        if time>updatetime:\n",
    "            updatebegintime = datetime.now()\n",
    "            update_predition_from_time(Prediction_score_dict,time,user_contribution2item)#use this score to do proactive caching\n",
    "            update_predition_from_time(Cache_score, time,user_contribution2item)  # use this score to do proactive caching\n",
    "            update_predition_from_time(Reactive_item_proactive_score,time,user_contribution2item)  # use this score to maintain proactive score\n",
    "            updatetime += timedelta(minutes=1)\n",
    "            updating_time_overhead_list.append((datetime.now()-updatebegintime).total_seconds())\n",
    "\n",
    "    #update link from last time to current time, so we can not update score on current time t, update_predition_from_time(Cache_score, time,user_contribution2item)\n",
    "        for i in range(len(bandwidth_link['item'])):\n",
    "\n",
    "            if bandwidth_link['end time'][0] < time:\n",
    "                req_time = bandwidth_link['begin time'].pop(0)\n",
    "                downloaoded_time = bandwidth_link['end time'].pop(0)\n",
    "                downloaded_item = bandwidth_link['item'].pop(0)                \n",
    "                proactive_indicator = bandwidth_link['proactive_indicator'].pop(0)\n",
    "                if downloaded_item == 'nan':\n",
    "                    print(downloaded_item,bandwidth_link,req_time,downloaded_item,proactive_indicator)\n",
    "#actually we do not need keey the score the latest for cache updating, we just user current score is ok, because we only care about current cache state\n",
    "#                 update_predition_from_time(Prediction_score_dict,downloaoded_time,user_contribution2item)  # use this score to do proactive caching\n",
    "#                 update_predition_from_time(Cache_score, downloaoded_time,user_contribution2item)  # use this score to do proactive caching\n",
    "\n",
    "                downloaded_item_score = check_score(Prediction_score_dict, downloaded_item,downloaoded_time)\n",
    "\n",
    "\n",
    "                #proactive and reactive cache\n",
    "                if proactive_indicator is True:\n",
    "#                     proactive_download += 1\n",
    "                    #go to proactive cache\n",
    "                    if len(proactive_cache) < P_CACHE_SIZE:\n",
    "                        # add item to cache\n",
    "                        proactive_cache.add(downloaded_item)\n",
    "                        record = Prediction_score_dict[downloaded_item]\n",
    "                        Cache_score[downloaded_item] = record\n",
    "                        del Prediction_score_dict[downloaded_item]\n",
    "                    elif downloaded_item not in proactive_cache and downloaded_item not in reactive_cache:\n",
    "                        # update cache\n",
    "                        lowest_item, lowest_score = get_lowest_content(Cache_score, downloaoded_time)\n",
    "                        if compare_score(downloaded_item_score, lowest_score):\n",
    "                            proactive_cache.add(downloaded_item)\n",
    "                            record = Prediction_score_dict[downloaded_item]\n",
    "                            Cache_score[downloaded_item] = record\n",
    "                            del Prediction_score_dict[downloaded_item]\n",
    "                            if len(proactive_cache) > P_CACHE_SIZE:\n",
    "                                proactive_cache.remove(lowest_item)\n",
    "                                record = Cache_score[lowest_item]\n",
    "                                Prediction_score_dict[lowest_item] = record\n",
    "                                del Cache_score[lowest_item]\n",
    "                    else:\n",
    "                        #hit\n",
    "                        pass\n",
    "\n",
    "\n",
    "                else:\n",
    "#                     reactive_download_number += 1\n",
    "                    #go to reactive cache\n",
    "                    if len(reactive_cache) < R_CACHE_SIZE:\n",
    "                        #add item to cache\n",
    "                        reactive_cache.add(downloaded_item)\n",
    "                        record = Prediction_score_dict[downloaded_item]\n",
    "                        Reactive_item_proactive_score[downloaded_item] = record\n",
    "                        del Prediction_score_dict[downloaded_item]\n",
    "                    elif downloaded_item not in reactive_cache and downloaded_item not in proactive_cache:\n",
    "                        #update reactive cache\n",
    "                        reactive_cache.add(downloaded_item)\n",
    "                        record = Prediction_score_dict[downloaded_item]\n",
    "                        Reactive_item_proactive_score[downloaded_item] = record\n",
    "                        del Prediction_score_dict[downloaded_item]\n",
    "\n",
    "\n",
    "                        position, kick_name, pasttimeforrank = get_lruK_action_rank(lruN, reactive_cache, past_timestamps,time)\n",
    "                        reactive_cache.remove(kick_name)\n",
    "                        record = Reactive_item_proactive_score[kick_name]\n",
    "                        Prediction_score_dict[kick_name] = record\n",
    "                        del Reactive_item_proactive_score[kick_name]\n",
    "\n",
    "                    else:\n",
    "                        #hit\n",
    "                        pass\n",
    "                if proactive_indicator is False:\n",
    "                    # for proactive latency, we calculate that in other part.\n",
    "                    # get reactive latency\n",
    "                    reactive_latency += (downloaoded_time - req_time).total_seconds()\n",
    "                    # print((downloaoded_time - req_time).total_seconds())\n",
    "\n",
    "    #update link from last time to current time, so we can not update score on current time t, update_predition_from_time(Cache_score, time,user_contribution2item)                \n",
    "        if time > link_empty_time:\n",
    "\n",
    "            #check error:\n",
    "            if len(bandwidth_link['item']) >0 :\n",
    "                print('error')\n",
    "        # we should have time to do proactive since link is empty\n",
    "        # update proactive score using last end time in link, keep score static from last end time to current time t1.\n",
    "#             update_predition_from_time(Prediction_score_dict,link_empty_time,user_contribution2item)#use this score to do proactive caching\n",
    "#             update_predition_from_time(Cache_score, link_empty_time,user_contribution2item)  # use this score to do proactive caching\n",
    "#             update_predition_from_time(Reactive_item_proactive_score,link_empty_time,user_contribution2item)  # use this score to maintain proactive score\n",
    "            highest_item, highest_score = get_highest_content(Prediction_score_dict,time)\n",
    "            lowest_item, lowest_score = get_lowest_content(Cache_score,time)\n",
    "            update_indicator = compare_score(highest_score,lowest_score)#if cache is full, we need indicator to update cache, if not full just do proactive cache\n",
    "\n",
    "            proaactive_count = 0\n",
    "            while(link_empty_time < time and (update_indicator or len(proactive_cache)< P_CACHE_SIZE) and highest_item is not None and proaactive_count < 1):\n",
    "                # bandwidth_link = {'begin time':[],'end time':[],'user':[],'proactive_indicator':[]}\n",
    "                bandwidth_link['begin time'].append(link_empty_time)\n",
    "                bandwidth_link['end time'].append(link_empty_time + timedelta(seconds=downloadtime))\n",
    "                bandwidth_link['item'].append(highest_item)\n",
    "                bandwidth_link['proactive_indicator'].append(True)\n",
    "                link_empty_time = link_empty_time + timedelta(seconds=downloadtime)\n",
    "                proactive_download += 1 #we count this from downloading start or end part\n",
    "                proaactive_count += 1\n",
    "\n",
    "                if link_empty_time < time:#link has been updated before proactive, so there should be only one item in the link\n",
    "                    if len(bandwidth_link['item']) != 1:\n",
    "                        print('error, not one proactive prefetching item')\n",
    "                    # download finish and replace cache\n",
    "                    bandwidth_link['begin time'].pop(0)\n",
    "                    bandwidth_link['end time'].pop(0)\n",
    "                    downloaded_item = bandwidth_link['item'].pop(0)\n",
    "                    proactive_indicator = bandwidth_link['proactive_indicator'].pop(0)\n",
    "#                     if proactive_indicator is True:                    \n",
    "#                         proactive_download += 1\n",
    "#                     else:\n",
    "#                         reactive_download_number += 1\n",
    "#                         print('error reactive indicator')\n",
    "\n",
    "                    proactive_cache.add(downloaded_item)\n",
    "                    record  = Prediction_score_dict[downloaded_item]\n",
    "                    Cache_score[downloaded_item] = record\n",
    "                    del Prediction_score_dict[downloaded_item]\n",
    "\n",
    "                    if len(proactive_cache)> P_CACHE_SIZE:\n",
    "                        proactive_cache.remove(lowest_item)\n",
    "                        record = Cache_score[lowest_item]\n",
    "                        Prediction_score_dict[lowest_item] = record\n",
    "                        del Cache_score[lowest_item]\n",
    "\n",
    "#                     update_predition_from_time(Prediction_score_dict,link_empty_time,user_contribution2item)\n",
    "#                     update_predition_from_time(Cache_score, link_empty_time,user_contribution2item)\n",
    "#                     update_predition_from_time(Reactive_item_proactive_score, link_empty_time,user_contribution2item)\n",
    "\n",
    "                    highest_item, highest_score = get_highest_content(Prediction_score_dict, time)\n",
    "                    lowest_item, lowest_score = get_lowest_content(Cache_score, time)\n",
    "                    update_indicator = compare_score(highest_score, lowest_score)\n",
    "                \n",
    "                \n",
    "\n",
    "        if item not in past_timestamps:\n",
    "            past_timestamps[item] = [time]\n",
    "        else:\n",
    "            past_timestamps[item].append(time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         update_predition_from_time(Prediction_score_dict,link_empty_time,user_contribution2item)\n",
    "#         update_predition_from_time(Cache_score, link_empty_time,user_contribution2item)\n",
    "#         update_predition_from_time(Reactive_item_proactive_score, link_empty_time,user_contribution2item)    \n",
    "\n",
    "\n",
    "        #make prediction\n",
    "        predicted_items = []\n",
    "        #-----------TV series prediction\n",
    "\n",
    "    #     print(item.split('(第'),'****')\n",
    "#         item = str(item)#avoid float item\n",
    "#         if len(item.split('(第'))>=2:\n",
    "#             try:\n",
    "#                 if re.search('集', item) and re.search('第', item) and len(re.findall(r\"\\d+\\.?\\d*\", item))>0:#not the full series like abc12,\n",
    "#                     episode_num = re.findall(r\"\\d+\\.?\\d*\", item)[-2]  # get the num-th of series, may need to change [-2] .mp4\n",
    "#         #             str(c).zfill(len(b))\n",
    "#                     predicted_series_item = re.sub(episode_num, str(int(episode_num)+1).zfill(len(episode_num)), item)\n",
    "#                     confidence = 1\n",
    "#                     predicted_items = [predicted_series_item]\n",
    "#                     predicted_scores = [1]\n",
    "#                     count_tvseries += 1\n",
    "#         #             print('predicted item',predicted_item,'item',item)\n",
    "#             except:#4. etc\n",
    "#                 count_other += 1\n",
    "#     #             pass\n",
    "        item = str(item)#avoid float item\n",
    "        if isseries(item):\n",
    "            predicted_series_item = predict_next_series(item)\n",
    "            confidence = 1\n",
    "            predicted_items = [predicted_series_item]\n",
    "            predicted_scores = [1]\n",
    "            count_tvseries += 1\n",
    "#             print(predicted_series_item,item)\n",
    "        elif user in usermappingdict and item  in itemmappingdict:\n",
    "            #no TV series but active user/item\n",
    "            \n",
    "            prediction_time_begin = datetime.now()\n",
    "            \n",
    "            count_activeother += 1\n",
    "            user = usermappingdict[user]\n",
    "            item = itemmappingdict[item]                \n",
    "            abs_time = (time - dataset_begintime).total_seconds()\n",
    "\n",
    "            #record user's history\n",
    "            if user not in ngram_dict:#new user\n",
    "                ngram_dict[user] = [item]\n",
    "                ngram_dict_time[user] = [abs_time]\n",
    "\n",
    "            else:\n",
    "                ngram_dict[user].append(item)\n",
    "                ngram_dict_time[user].append(abs_time)\n",
    "\n",
    "\n",
    "            seq = np.zeros([args.maxlen], dtype=np.int32)\n",
    "            time_seq = np.zeros([args.maxlen], dtype=np.int32)\n",
    "\n",
    "            seq[-len(ngram_dict[user]):] = ngram_dict[user][-args.maxlen:]\n",
    "            time_seq[-len(ngram_dict_time[user]):] = ngram_dict_time[user][-args.maxlen:]\n",
    "\n",
    "    #         print(seq,item,time_seq,abs_time)         \n",
    "\n",
    "    #----------------------- ngram prediction and save the confident coefficient\n",
    "            ngram_dict_list = list(seq)  \n",
    "            predicted_item_list,confidence,series_indicator = prediction_topk(ngram_dict_list[-N:], treeroot,preditivemodeltopk)\n",
    "    #         confidence = [float(i)/sum(confidence) for i in confidence]\n",
    "        #     print(predicted_item,confidence)\n",
    "        #     predicted_item = predicted_item_list[0]\n",
    "    #----------------------- Tisasrec prediction and save the confident coefficient\n",
    "    #the following is for Tisasrec model:\n",
    "            \n",
    "            item_idx = []\n",
    "            for t in range(itemnum+1):\n",
    "                item_idx.append(t)        \n",
    "            time_matrix = computeRePos(time_seq, args.time_span)\n",
    "            predictions_t = -model.predict(sess, [u], [seq], [time_matrix],item_idx)       \n",
    "            predictions_t = predictions_t[0]\n",
    "            rank_raw0 = predictions_t.argsort()\n",
    "            top10t = rank_raw0[0:preditivemodeltopk]\n",
    "            top10t_item_name = []\n",
    "            top10t_item_score = []\n",
    "            for _ in top10t:\n",
    "                item_index = item_idx[_]#item name\n",
    "                top10t_item_name.append(item_index)\n",
    "                top10t_item_score.append(predictions_t[item_index])   \n",
    "    #the following is fusion\n",
    "    #the following is for fusion combsum\n",
    "            r1 = top10t_item_name\n",
    "            r2 = predicted_item_list\n",
    "            s1 = [-v for v in top10t_item_score]\n",
    "            s2 = confidence\n",
    "            new_rank_combsum,new_rank_score = combsum(r1,r2,s1,s2,fusiontopk)\n",
    "\n",
    "            #get back to original name\n",
    "            user = usermappingdict[user]\n",
    "            item = itemmappingdict[item] \n",
    "\n",
    "\n",
    "\n",
    "            for encodeditem in new_rank_combsum:            \n",
    "                predicted_items.append(itemmappingdict[encodeditem])\n",
    "            predicted_scores = new_rank_score\n",
    "            \n",
    "            prediction_time_overhead_list.append((datetime.now()-prediction_time_begin).total_seconds())\n",
    "        else:\n",
    "            count_other += 1\n",
    "            #do not predict\n",
    "    #         pass\n",
    "\n",
    "\n",
    "    #the following is record user's contribution to item\n",
    "        for predicted_item in predicted_items:\n",
    "            if user not in user_contribution2item:\n",
    "                user_contribution2item[user] = [predicted_item]\n",
    "            else:\n",
    "                user_contribution2item[user].append(predicted_item)\n",
    "\n",
    "\n",
    "    # the following is caching calculaton    \n",
    "        if item in proactive_cache:\n",
    "            hit += 1\n",
    "            accum_hit += 1\n",
    "#             update_predition_from_time(Cache_score, time,user_contribution2item)\n",
    "            strip_the_served_score(Cache_score,time,item,user)\n",
    "\n",
    "    #         print(item in Cache_score, item in user_contribution2item[user])        \n",
    "            try:\n",
    "                user_contribution2item[user].remove(item)\n",
    "            except:\n",
    "                pass\n",
    "    #             print('user_contribution2item no record')\n",
    "    #             print(Cache_score[item])\n",
    "\n",
    "\n",
    "        elif item in reactive_cache:\n",
    "            hit += 1\n",
    "            accum_hit += 1\n",
    "#             update_predition_from_time(Reactive_item_proactive_score, time,user_contribution2item)\n",
    "            strip_the_served_score(Reactive_item_proactive_score,time,item,user)\n",
    "            try:\n",
    "                user_contribution2item[user].remove(item)\n",
    "            except:\n",
    "                pass\n",
    "    #             print('no proactive score')\n",
    "\n",
    "\n",
    "\n",
    "        elif item in bandwidth_link['item']:\n",
    "            soft_hit += 1\n",
    "            if bandwidth_link['proactive_indicator'][bandwidth_link['item'].index(item)] is True:\n",
    "                proactive_latency += (bandwidth_link['end time'][bandwidth_link['item'].index(item)] - time).total_seconds()#this is the only proactive latency\n",
    "#                 update_predition_from_time(Prediction_score_dict, time,user_contribution2item)\n",
    "                strip_the_served_score(Prediction_score_dict,time,item,user)\n",
    "                try:\n",
    "                    user_contribution2item[user].remove(item)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            #not hit and download\n",
    "            bandwidth_link['begin time'].append(time)\n",
    "            bandwidth_link['end time'].append( max(link_empty_time,time) + timedelta(seconds=downloadtime))\n",
    "            bandwidth_link['item'].append(item)\n",
    "            bandwidth_link['proactive_indicator'].append(False)\n",
    "            reactive_download_number += 1\n",
    "            link_empty_time  = max(link_empty_time,time) + timedelta(seconds=downloadtime)\n",
    "            # need to strip off user's score of past item\n",
    "            if user not in user_contribution2item:\n",
    "                user_contribution2item[user] = []\n",
    "            if len(user_contribution2item[user])>0:\n",
    "                for item_from_this_user in user_contribution2item[user].copy():\n",
    "\n",
    "                    if item_from_this_user in Prediction_score_dict:            \n",
    "                        strip_the_served_score(Prediction_score_dict,time,item_from_this_user,user)\n",
    "                    if item_from_this_user in Reactive_item_proactive_score:            \n",
    "                        strip_the_served_score(Reactive_item_proactive_score,time,item_from_this_user,user)\n",
    "                    if item_from_this_user in Cache_score:\n",
    "                        strip_the_served_score(Cache_score,time,item_from_this_user,user)                                        \n",
    "                    user_contribution2item[user].remove(item_from_this_user)\n",
    "\n",
    "#assign score(this should after strip!!!!!!)                    \n",
    "#         est_time = guess_len\n",
    "#         duriation = guess_len\n",
    "        try:\n",
    "            if medium_std_dict[item][1] < 40:                \n",
    "                duriation = float(medium_std_dict[item][1])\n",
    "            else:\n",
    "                duriation = 40\n",
    "            if medium_std_dict[item][0] > 0:\n",
    "                est_time = float(medium_std_dict[item][0])\n",
    "#             else:\n",
    "#                 est_time = guess_len\n",
    "        except:\n",
    "            est_time = guess_len\n",
    "            duriation = guess_len\n",
    "        if duriation == 0:\n",
    "            duriation = guess_len\n",
    "\n",
    "    #     predit_time_begin = time + timedelta(minutes=est_time / k1)\n",
    "    #     predit_time_end = time + timedelta(minutes=est_time/ k1 * k2)\n",
    "        predit_time_begin = time + timedelta(minutes=est_time) - timedelta(minutes=duriation/2)\n",
    "        predit_time_end = time + timedelta(minutes=est_time) + timedelta(minutes=duriation/2)\n",
    "        content_score = SCORE_Norm / ((predit_time_end - predit_time_begin).total_seconds() / 60)\n",
    "        for j,predicted_item in enumerate(predicted_items):\n",
    "            #assigning predictive score             \n",
    "            if predicted_item not in proactive_cache and predicted_item not in reactive_cache:\n",
    "                # Prediction_score_dict[predicted_item] the False is for update_predition_from_time(Prediction_score_dict,link_empty_time) to indicate whether the score is increased of not\n",
    "                if predicted_item not in Prediction_score_dict:\n",
    "                    Prediction_score_dict[predicted_item] = [[predit_time_begin, predit_time_end, content_score*predicted_scores[j],False,user]]\n",
    "                else:\n",
    "                    Prediction_score_dict[predicted_item].append([predit_time_begin, predit_time_end, content_score*predicted_scores[j],False,user])\n",
    "            elif predicted_item in proactive_cache:\n",
    "                # more than one contributor for this content score\n",
    "                Cache_score[predicted_item].append([predit_time_begin, predit_time_end, content_score*predicted_scores[j],False,user])\n",
    "            else:\n",
    "                Reactive_item_proactive_score[predicted_item].append([predit_time_begin, predit_time_end, content_score*predicted_scores[j],False,user])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if total_count % printoutfrequency == 0:\n",
    "            hit_rate_list.append(hit/local_count)\n",
    "            proactive_download_list.append(proactive_download)\n",
    "            timedelta_frequency = (time - calculatetime).total_seconds()\n",
    "            calculatetime = time\n",
    "            print('delta',timedelta_frequency)\n",
    "            print('updating time',sum(updating_time_overhead_list)/len(updating_time_overhead_list))\n",
    "            print('prediction time',sum(prediction_time_overhead_list)/len(prediction_time_overhead_list))\n",
    "#----------------------bandwidth utilization\n",
    "            \n",
    "            reactive_number_in_bandwidth = 0\n",
    "            proactive_number_in_bandwidth = 0\n",
    "            for i in range(len(bandwidth_link['item'])):\n",
    "                if bandwidth_link['proactive_indicator'][i]:\n",
    "                    proactive_number_in_bandwidth += 1\n",
    "                else:\n",
    "                    reactive_number_in_bandwidth += 1\n",
    "#             print('# in link',proactive_number_in_bandwidth,reactive_number_in_bandwidth)\n",
    "            #(number of local_count - reactive number in bandwidth)*downloadtime / time = reactive utilization\n",
    "#             print('reactive number',local_count-reactive_number_in_bandwidth)\n",
    "\n",
    "#             print('reactive donwload number',reactive_download_number)\n",
    "#             print('proactive donwload number',proactive_download)\n",
    "#                 print('reactive in link',reactive_number_in_bandwidth)\n",
    "#                 print('roactive in link',proactive_number_in_bandwidth)\n",
    "            reactive_utilization = round((reactive_download_number - reactive_number_in_bandwidth)*downloadtime / timedelta_frequency,4)\n",
    "            utilization_list_reactive.append(reactive_utilization)\n",
    "            \n",
    "            #(number of proactive download - proactive number in bandwidth)*downloadtime / time = proactive utilization\n",
    "            proactive_utilization = round((proactive_download - proactive_number_in_bandwidth)*downloadtime / timedelta_frequency,4)\n",
    "            utilization_list_proactive.append(proactive_utilization)\n",
    "            \n",
    "            utilization_list_all.append(reactive_utilization + proactive_utilization)\n",
    "            print(round(hit / local_count, 4),'proactive download',proactive_download,soft_hit,'buffer content',len(bandwidth_link['item']))            \n",
    "#             print('utilization',reactive_utilization + proactive_utilization, reactive_utilization, proactive_utilization)\n",
    "#             print(round(hit / local_count, 4),datetime.now()-last_time)        \n",
    "#             print(time)\n",
    "            last_time = datetime.now()\n",
    "            hit = 0\n",
    "            local_count = 0\n",
    "            proactive_download = 0\n",
    "            reactive_download_number =0\n",
    "            soft_hit = 0\n",
    "            count_tvseries = 0\n",
    "            count_activeother = 0\n",
    "            count_other = 0\n",
    "            \n",
    "#         if total_count == stop_point:\n",
    "        if time >= stop_time:\n",
    "    #         hit_rate_list.append(hit/local_count)\n",
    "    #         proactive_download_list.append(proactive_download)\n",
    "    #         print(round(hit / local_count, 4),'proactive download',proactive_download,soft_hit,'buffer content',len(bandwidth_link['item']))\n",
    "            print(hit_rate_list)\n",
    "            print(proactive_download_list)\n",
    "            print('proactive utilization',utilization_list_proactive)\n",
    "            print('reactive utilization',utilization_list_reactive)\n",
    "            print('all utilization',utilization_list_all)\n",
    "            print(round(accum_hit / total_count, 4), 'latency', proactive_latency + reactive_latency)\n",
    "            print('updating time',sum(updating_time_overhead_list)/len(updating_time_overhead_list))\n",
    "            print('prediction time',sum(prediction_time_overhead_list)/len(prediction_time_overhead_list))\n",
    "            break\n",
    "\n",
    "            # x = range(len(hit_rate_list))\n",
    "            # plt.plot(x, hit_rate_list)\n",
    "            # plt.title('proactive caching')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "        #             return proactive_latency + reactive_latency, hit_rate_list,proactive_download_list\n",
    "\n",
    "\n",
    "\n",
    "    #     print(hit_rate_list)\n",
    "    #     print (round(accum_hit / total_count,4),'latency',proactive_latency+reactive_latency,'re',reactive_latency,'pro',proactive_latency)\n",
    "        # x = range(len(hit_rate_list))\n",
    "        # plt.plot(x, hit_rate_list)\n",
    "        # plt.title('proactive caching')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "        # return proactive_latency+reactive_latency,hit_rate_list,proactive_download_list\n",
    "    return \n",
    "a = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d287545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T04:09:09.510086Z",
     "start_time": "2022-02-09T03:44:41.585843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p cache size: 1800 r cache size: 200 frequency 5000 downloadtime 1 lruN 2\n",
      "begintime 2010-07-05 00:00:00 stoptime 2010-07-07 00:00:00\n",
      "delta 15103.0\n",
      "updating time 0.0036915436507936553\n",
      "prediction time 0.028773328472222262\n",
      "0.4454 proactive download 3257 28 buffer content 0\n",
      "delta 25464.0\n",
      "updating time 0.003547270310192035\n",
      "prediction time 0.028885607348703247\n",
      "0.5522 proactive download 3831 94 buffer content 1\n",
      "delta 9216.0\n",
      "updating time 0.0042045590361445935\n",
      "prediction time 0.029041359636537506\n",
      "0.5166 proactive download 2969 78 buffer content 1\n",
      "delta 8630.0\n",
      "updating time 0.004875332648870643\n",
      "prediction time 0.02923775224195533\n",
      "0.525 proactive download 2928 68 buffer content 1\n",
      "delta 8106.0\n",
      "updating time 0.005499920649233544\n",
      "prediction time 0.029181616282379644\n",
      "0.4944 proactive download 2692 56 buffer content 1\n",
      "delta 6355.0\n",
      "updating time 0.00608566008230453\n",
      "prediction time 0.029660803516998784\n",
      "0.4252 proactive download 2017 101 buffer content 2\n",
      "delta 5002.0\n",
      "updating time 0.006629986902927582\n",
      "prediction time 0.029928037393483563\n",
      "0.4174 proactive download 1400 63 buffer content 3\n",
      "delta 4034.0\n",
      "updating time 0.007176223279648613\n",
      "prediction time 0.03001255390174628\n",
      "0.3564 proactive download 652 74 buffer content 3\n",
      "delta 4854.0\n",
      "updating time 0.00774229025570145\n",
      "prediction time 0.030021790885535513\n",
      "0.3826 proactive download 1216 104 buffer content 2\n",
      "delta 11428.0\n",
      "updating time 0.00842524496029322\n",
      "prediction time 0.03005374031220025\n",
      "0.515 proactive download 3061 59 buffer content 1\n",
      "delta 27126.0\n",
      "updating time 0.009164276687410242\n",
      "prediction time 0.03014764159789928\n",
      "0.5176 proactive download 3985 88 buffer content 0\n",
      "delta 8400.0\n",
      "updating time 0.009539493943472404\n",
      "prediction time 0.030223954020359024\n",
      "0.507 proactive download 2792 77 buffer content 3\n",
      "delta 8113.0\n",
      "updating time 0.00995001142131979\n",
      "prediction time 0.030276839832285025\n",
      "0.4984 proactive download 2760 71 buffer content 1\n",
      "delta 7861.0\n",
      "updating time 0.010340540280561103\n",
      "prediction time 0.03029981695290309\n",
      "0.529 proactive download 2823 36 buffer content 1\n",
      "delta 7090.0\n",
      "updating time 0.010685328232593707\n",
      "prediction time 0.03040231381880207\n",
      "0.465 proactive download 2426 86 buffer content 4\n",
      "delta 5222.0\n",
      "updating time 0.010999697519437232\n",
      "prediction time 0.03045318016806712\n",
      "0.4222 proactive download 1574 113 buffer content 1\n",
      "delta 4359.0\n",
      "updating time 0.011303977280923175\n",
      "prediction time 0.030500703352109228\n",
      "0.4238 proactive download 1112 72 buffer content 3\n",
      "delta 4528.0\n",
      "updating time 0.01161825026325025\n",
      "prediction time 0.0305075278522489\n",
      "0.3676 proactive download 979 85 buffer content 1\n",
      "[0.4454, 0.5522, 0.5166, 0.525, 0.4944, 0.4252, 0.4174, 0.3564, 0.3826, 0.515, 0.5176, 0.507, 0.4984, 0.529, 0.465, 0.4222, 0.4238, 0.3676]\n",
      "[3257, 3831, 2969, 2928, 2692, 2017, 1400, 652, 1216, 3061, 3985, 2792, 2760, 2823, 2426, 1574, 1112, 979]\n",
      "proactive utilization [0.2157, 0.1504, 0.3222, 0.3393, 0.3321, 0.3174, 0.2799, 0.1616, 0.2505, 0.2678, 0.1469, 0.3324, 0.3402, 0.3591, 0.3422, 0.3014, 0.2551, 0.216]\n",
      "reactive utilization [0.1818, 0.0842, 0.2537, 0.2672, 0.3048, 0.436, 0.5692, 0.7786, 0.6141, 0.207, 0.0857, 0.2839, 0.3003, 0.2949, 0.3646, 0.5314, 0.6437, 0.6795]\n",
      "all utilization [0.39749999999999996, 0.2346, 0.5759, 0.6065, 0.6369, 0.7534000000000001, 0.8491, 0.9401999999999999, 0.8646, 0.4748, 0.2326, 0.6163, 0.6405000000000001, 0.6539999999999999, 0.7068, 0.8328, 0.8988, 0.8955]\n",
      "0.4632 latency 95759.0\n",
      "updating time 0.011741553819444433\n",
      "prediction time 0.030522500437253915\n"
     ]
    }
   ],
   "source": [
    "CACHE_SIZE_LIST = [2000]\n",
    "for size in CACHE_SIZE_LIST:\n",
    "    for downloadtime1 in [1]:\n",
    "        PEC(CACHE_SIZE = size, ratio = 0.9, lruN = 2,downloadtime = downloadtime1,printoutfrequency = 5000,begintime = datetime(2010,7,5),stop_time = datetime(2010,7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9ea0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
